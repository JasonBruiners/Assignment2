---
title: "Data Extraction"
output: html_notebook
---

```{r}
#install_github("mkearney/rtweet")

#Packages
library(devtools)
library(rtweet)
library(readr)
library(usethis)
library(topicmodels)
```

```{r}

#Define credentials
app_name<-""
consumer_key<-""
consumer_secret<-""
access_token<-""
access_token_secret<-""


```

## checking rate limits
```{r}
# see rate limits
# The rate limit for the standard search API is 18,000 tweets per fifteen minutes. With a bearer token, the rate limit is 45,000 tweets per fifteen minutes. 
rate_limits<-rate_limit()
head(rate_limits[,1:4])
```

```{r}

```


```{r}
#
create_token(app=app_name,
             consumer_key=consumer_key, consumer_secret=consumer_secret,
             access_token = access_token, access_secret = access_token_secret)


#Extracts 3000 tweets with appropriate hashtag
NEWS24_tweets<-search_tweets("", n=50, include_rts = FALSE)
ENCA_tweets<-search_tweets("", n=50, include_rts = FALSE)
#TOPIC_tweets<-search_tweets("", n=50, include_rts = FALSE)

```

## Searching for tweets...
### Collect first batch of tweets
```{r echo=TRUE, message=FALSE, warning=FALSE}

News24_tweets <- get_timelines(c("News24"), n = 3200)
TimesLIVE_tweets <- get_timelines(c("TimesLIVE"), n = 3200)
eNCA_tweets <- get_timelines(c("eNCA"), n = 3200)

```

### Collect second batch of tweets
```{r echo=TRUE, message=FALSE, warning=FALSE}
IOL_tweets <- get_timelines(c("IOL"), n=3200)
SABCNews_tweets <- get_timelines(c("SABCNews"), n=3200)
ewnupdates_tweets <- get_timelines(c("ewnupdates"), n=3200)
```

### Collect third batch of tweets
```{r echo=TRUE, message=FALSE, warning=FALSE}
#Not useful
#trend_tweets <- get_trends("South Africa")
#trend_tweets
```

### Collect fourth batch of tweets
```{r echo=TRUE, message=FALSE, warning=FALSE}
#Collecting last 18000 tweets for South Africa
```

### Collect fifth batch of tweets
```{r echo=TRUE, message=FALSE, warning=FALSE}
#Collecting last 18000 tweets with the Corona list search terms for South Africa
```

## finished collecting tweets!


# Performing transformations
```{r}
# Sentiment Analysis


```

```{r}
# Topic Analysis


```

```{r}
# Other Analysis

# Frequency of tweets
News24_freq <- separate(News24_tweets, created_at, into=c("date","time"), sep=" ") %>% select(date, screen_name)

timesLIVE_freq <- separate(TimesLIVE_tweets, created_at, into=c("date","time"), sep=" ") %>% select(date, screen_name)

Ewnupdate_freq <- separate(News24_tweets, created_at, into=c("date","time"), sep=" ") %>% select(date, screen_name)

IOL_freq <- separate(IOL_tweets, created_at, into=c("date","time"), sep=" ") %>% select(date, screen_name)

eNCA_freq <- separate(ewnupdates_tweets, created_at, into=c("date","time"), sep=" ") %>% select(date, screen_name)

SABC_freq <- separate(SABCNews_tweets, created_at, into=c("date","time"), sep=" ") %>% select(date, screen_name)
```

## Done

# Exporting results
```{r echo=TRUE}
write_rds(News24_tweets, "Data/News24_tweets.rds")
write_rds(TimesLIVE_tweets, "Data/TimesLIVE_tweets.rds")
write_rds(eNCA_tweets, "Data/eNCA_tweets.rds")
write_rds(IOL_tweets,"Data/IOL_tweets.rds")
write_rds(SABCNews_tweets,"Data/SABCNews_tweets.rds")
write_rds(ewnupdates_tweets,"Data/ewnupdates_tweets")
```

```{r}




```



```

## Exporting results
```{r}

```

