---
title: "Data Transformation"
output: html_notebook
---

```{r}
#install_github("mkearney/rtweet")

#Packages
library(usethis)
library(devtools)
library(dplyr)
library(rtweet)
library(readr)
library(topicmodels)
library(tidyverse)
library(maps)
library(tidytext)
library(textdata)
library(tm)
library(stm)
```

## Load extracted data 
```{r}
News24_tweets <- read_rds("Data/Raw_Timeline_Data/News24_tweets.rds")
TimesLIVE_tweets <- read_rds("Data/Raw_Timeline_Data/TimesLIVE_tweets.rds")
eNCA_tweets <- read_rds("Data/Raw_Timeline_Data/eNCA_tweets.rds")
IOL_tweets <- read_rds("Data/Raw_Timeline_Data/IOL_tweets.rds")
SABCNews_tweets <- read_rds("Data/Raw_Timeline_Data/SABCNews_tweets.rds")
ewnupdates_tweets <- read_rds("Data/Raw_Timeline_Data/ewnupdates_tweets.rds")
```

```{r}
tweets_za_13 <- read_rds("Data/Raw_Filtered_Data/tweets_za_13.rds")
tweets_za_14 <- read_rds("Data/Raw_Filtered_Data/tweets_za_14.rds")
tweets_za_15 <- read_rds("Data/Raw_Filtered_Data/tweets_za_15.rds")
tweets_za_16 <- read_rds("Data/Raw_Filtered_Data/tweets_za_16.rds")
tweets_za_17 <- read_rds("Data/Raw_Filtered_Data/tweets_za_17.rds")
tweets_za_18 <- read_rds("Data/Raw_Filtered_Data/tweets_za_18.rds")
tweets_za_19 <- read_rds("Data/Raw_Filtered_Data/tweets_za_19.rds")
tweets_za_20 <- read_rds("Data/Raw_Filtered_Data/tweets_za_20.rds")
tweets_za_21 <- read_rds("Data/Raw_Filtered_Data/tweets_za_21.rds")
#tweets_za_22 <- read_rds("Data/Raw_Filtered_Data/tweets_za_22.rds")
```

```{r}
tweets_bloem_13 <- read_rds("Data/Raw_Filtered_Data/tweets_bloem_13.rds")
tweets_bloem_14 <- read_rds("Data/Raw_Filtered_Data/tweets_bloem_14.rds")
tweets_bloem_15 <- read_rds("Data/Raw_Filtered_Data/tweets_bloem_15.rds")
tweets_bloem_16 <- read_rds("Data/Raw_Filtered_Data/tweets_bloem_16.rds")
tweets_bloem_17 <- read_rds("Data/Raw_Filtered_Data/tweets_bloem_17.rds")
tweets_bloem_18 <- read_rds("Data/Raw_Filtered_Data/tweets_bloem_18.rds")
tweets_bloem_19 <- read_rds("Data/Raw_Filtered_Data/tweets_bloem_19.rds")
tweets_bloem_20 <- read_rds("Data/Raw_Filtered_Data/tweets_bloem_20.rds")
tweets_bloem_21 <- read_rds("Data/Raw_Filtered_Data/tweets_bloem_21.rds")
#tweets_bloem_22 <- read_rds("Data/Raw_Filtered_Data/tweets_bloem_22.rds")
```

```{r}
tweets_cpt_13 <- read_rds("Data/Raw_Filtered_Data/tweets_cpt_13.rds")
tweets_cpt_14 <- read_rds("Data/Raw_Filtered_Data/tweets_cpt_14.rds")
tweets_cpt_15 <- read_rds("Data/Raw_Filtered_Data/tweets_cpt_15.rds")
tweets_cpt_16 <- read_rds("Data/Raw_Filtered_Data/tweets_cpt_16.rds")
tweets_cpt_17 <- read_rds("Data/Raw_Filtered_Data/tweets_cpt_17.rds")
tweets_cpt_18 <- read_rds("Data/Raw_Filtered_Data/tweets_cpt_18.rds")
tweets_cpt_19 <- read_rds("Data/Raw_Filtered_Data/tweets_cpt_19.rds")
tweets_cpt_20 <- read_rds("Data/Raw_Filtered_Data/tweets_cpt_20.rds")
tweets_cpt_21 <- read_rds("Data/Raw_Filtered_Data/tweets_cpt_21.rds")
#tweets_cpt_22 <- read_rds("Data/Raw_Filtered_Data/tweets_cpt_22.rds")
```

```{r}
tweets_durb_13 <- read_rds("Data/Raw_Filtered_Data/tweets_durb_13.rds")
tweets_durb_14 <- read_rds("Data/Raw_Filtered_Data/tweets_durb_14.rds")
tweets_durb_15 <- read_rds("Data/Raw_Filtered_Data/tweets_durb_15.rds")
tweets_durb_16 <- read_rds("Data/Raw_Filtered_Data/tweets_durb_16.rds")
tweets_durb_17 <- read_rds("Data/Raw_Filtered_Data/tweets_durb_17.rds")
tweets_durb_18 <- read_rds("Data/Raw_Filtered_Data/tweets_durb_18.rds")
tweets_durb_19 <- read_rds("Data/Raw_Filtered_Data/tweets_durb_19.rds")
tweets_durb_20 <- read_rds("Data/Raw_Filtered_Data/tweets_durb_20.rds")
tweets_durb_21 <- read_rds("Data/Raw_Filtered_Data/tweets_durb_21.rds")
#tweets_durb_22 <- read_rds("Data/Raw_Filtered_Data/tweets_durb_22.rds")
```

```{r}
tweets_jhb_13 <- read_rds("Data/Raw_Filtered_Data/tweets_jhb_13.rds")
tweets_jhb_14 <- read_rds("Data/Raw_Filtered_Data/tweets_jhb_14.rds")
tweets_jhb_15 <- read_rds("Data/Raw_Filtered_Data/tweets_jhb_15.rds")
tweets_jhb_16 <- read_rds("Data/Raw_Filtered_Data/tweets_jhb_16.rds")
tweets_jhb_17 <- read_rds("Data/Raw_Filtered_Data/tweets_jhb_17.rds")
tweets_jhb_18 <- read_rds("Data/Raw_Filtered_Data/tweets_jhb_18.rds")
tweets_jhb_19 <- read_rds("Data/Raw_Filtered_Data/tweets_jhb_19.rds")
tweets_jhb_20 <- read_rds("Data/Raw_Filtered_Data/tweets_jhb_20.rds")
tweets_jhb_21 <- read_rds("Data/Raw_Filtered_Data/tweets_jhb_21.rds")
#tweets_jhb_22 <- read_rds("Data/Raw_Filtered_Data/tweets_jhb_22.rds")
```

```{r}
tweets_kim_13 <- read_rds("Data/Raw_Filtered_Data/tweets_kim_13.rds")
tweets_kim_14 <- read_rds("Data/Raw_Filtered_Data/tweets_kim_14.rds")
tweets_kim_15 <- read_rds("Data/Raw_Filtered_Data/tweets_kim_15.rds")
tweets_kim_16 <- read_rds("Data/Raw_Filtered_Data/tweets_kim_16.rds")
tweets_kim_17 <- read_rds("Data/Raw_Filtered_Data/tweets_kim_17.rds")
tweets_kim_18 <- read_rds("Data/Raw_Filtered_Data/tweets_kim_18.rds")
tweets_kim_19 <- read_rds("Data/Raw_Filtered_Data/tweets_kim_19.rds")
tweets_kim_20 <- read_rds("Data/Raw_Filtered_Data/tweets_kim_20.rds")
tweets_kim_21 <- read_rds("Data/Raw_Filtered_Data/tweets_kim_21.rds")
#tweets_kim_22 <- read_rds("Data/Raw_Filtered_Data/tweets_kim_22.rds")
```

```{r}
tweets_klerk_13 <- read_rds("Data/Raw_Filtered_Data/tweets_klerk_13.rds")
tweets_klerk_14 <- read_rds("Data/Raw_Filtered_Data/tweets_klerk_14.rds")
tweets_klerk_15 <- read_rds("Data/Raw_Filtered_Data/tweets_klerk_15.rds")
tweets_klerk_16 <- read_rds("Data/Raw_Filtered_Data/tweets_klerk_16.rds")
tweets_klerk_17 <- read_rds("Data/Raw_Filtered_Data/tweets_klerk_17.rds")
tweets_klerk_18 <- read_rds("Data/Raw_Filtered_Data/tweets_klerk_18.rds")
tweets_klerk_19 <- read_rds("Data/Raw_Filtered_Data/tweets_klerk_19.rds")
tweets_klerk_20 <- read_rds("Data/Raw_Filtered_Data/tweets_klerk_20.rds")
tweets_klerk_21 <- read_rds("Data/Raw_Filtered_Data/tweets_klerk_21.rds")
#tweets_klerk_22 <- read_rds("Data/Raw_Filtered_Data/tweets_klerk_22.rds")
```

```{r}
tweets_mbomb_13 <- read_rds("Data/Raw_Filtered_Data/tweets_mbomb_13.rds")
tweets_mbomb_14 <- read_rds("Data/Raw_Filtered_Data/tweets_mbomb_14.rds")
tweets_mbomb_15 <- read_rds("Data/Raw_Filtered_Data/tweets_mbomb_15.rds")
tweets_mbomb_16 <- read_rds("Data/Raw_Filtered_Data/tweets_mbomb_16.rds")
tweets_mbomb_17 <- read_rds("Data/Raw_Filtered_Data/tweets_mbomb_17.rds")
tweets_mbomb_18 <- read_rds("Data/Raw_Filtered_Data/tweets_mbomb_18.rds")
tweets_mbomb_19 <- read_rds("Data/Raw_Filtered_Data/tweets_mbomb_19.rds")
tweets_mbomb_20 <- read_rds("Data/Raw_Filtered_Data/tweets_mbomb_20.rds")
tweets_mbomb_21 <- read_rds("Data/Raw_Filtered_Data/tweets_mbomb_21.rds")
#tweets_mbomb_22 <- read_rds("Data/Raw_Filtered_Data/tweets_mbomb_22.rds")
```

```{r}
tweets_polo_13 <- read_rds("Data/Raw_Filtered_Data/tweets_polo_13.rds")
tweets_polo_14 <- read_rds("Data/Raw_Filtered_Data/tweets_polo_14.rds")
tweets_polo_15 <- read_rds("Data/Raw_Filtered_Data/tweets_polo_15.rds")
tweets_polo_16 <- read_rds("Data/Raw_Filtered_Data/tweets_polo_16.rds")
tweets_polo_17 <- read_rds("Data/Raw_Filtered_Data/tweets_polo_17.rds")
tweets_polo_18 <- read_rds("Data/Raw_Filtered_Data/tweets_polo_18.rds")
tweets_polo_19 <- read_rds("Data/Raw_Filtered_Data/tweets_polo_19.rds")
tweets_polo_20 <- read_rds("Data/Raw_Filtered_Data/tweets_polo_20.rds")
tweets_polo_21 <- read_rds("Data/Raw_Filtered_Data/tweets_polo_21.rds")
#tweets_polo_22 <- read_rds("Data/Raw_Filtered_Data/tweets_polo_22.rds")
```

```{r}
tweets_pe_13 <- read_rds("Data/Raw_Filtered_Data/tweets_pe_13.rds")
tweets_pe_14 <- read_rds("Data/Raw_Filtered_Data/tweets_pe_14.rds")
tweets_pe_15 <- read_rds("Data/Raw_Filtered_Data/tweets_pe_15.rds")
tweets_pe_16 <- read_rds("Data/Raw_Filtered_Data/tweets_pe_16.rds")
tweets_pe_17 <- read_rds("Data/Raw_Filtered_Data/tweets_pe_17.rds")
tweets_pe_18 <- read_rds("Data/Raw_Filtered_Data/tweets_pe_18.rds")
tweets_pe_19 <- read_rds("Data/Raw_Filtered_Data/tweets_pe_19.rds")
tweets_pe_20 <- read_rds("Data/Raw_Filtered_Data/tweets_pe_20.rds")
tweets_pe_21 <- read_rds("Data/Raw_Filtered_Data/tweets_pe_21.rds")
#tweets_pe_22 <- read_rds("Data/Raw_Filtered_Data/tweets_pe_22.rds")
```

# Bind Dataframes
```{r}
tweets_za <-rbind(tweets_za_21,tweets_za_20,tweets_za_19,tweets_za_18,tweets_za_17,tweets_za_16,tweets_za_15,tweets_za_14,tweets_za_13)
tweets_bloem <-rbind(tweets_bloem_21,tweets_bloem_20,tweets_bloem_19,tweets_bloem_18,tweets_bloem_17,tweets_bloem_16,tweets_bloem_15,tweets_bloem_14,tweets_bloem_13)
tweets_cpt <-rbind(tweets_cpt_21,tweets_cpt_20,tweets_cpt_19,tweets_cpt_18,tweets_cpt_17,tweets_cpt_16,tweets_cpt_15,tweets_cpt_14,tweets_cpt_13)
tweets_durb <-rbind(tweets_durb_21,tweets_durb_20,tweets_durb_19,tweets_durb_18,tweets_durb_17,tweets_durb_16,tweets_durb_15,tweets_durb_14,tweets_durb_13)
tweets_jhb <-rbind(tweets_jhb_21,tweets_jhb_20,tweets_jhb_19,tweets_jhb_18,tweets_jhb_17,tweets_jhb_16,tweets_jhb_15,tweets_jhb_14,tweets_jhb_13)
tweets_kim <-rbind(tweets_kim_21,tweets_kim_20,tweets_kim_19,tweets_kim_18,tweets_kim_17,tweets_kim_16,tweets_kim_15,tweets_kim_14,tweets_kim_13)
tweets_klerk <-rbind(tweets_klerk_21,tweets_klerk_20,tweets_klerk_19,tweets_klerk_18,tweets_klerk_17,tweets_klerk_16,tweets_klerk_15,tweets_klerk_14,tweets_klerk_13)
tweets_mbomb <-rbind(tweets_mbomb_21,tweets_mbomb_20,tweets_mbomb_19,tweets_mbomb_18,tweets_mbomb_17,tweets_mbomb_16,tweets_mbomb_15,tweets_mbomb_14,tweets_mbomb_13)
tweets_polo <-rbind(tweets_polo_21,tweets_polo_20,tweets_polo_19,tweets_polo_18,tweets_polo_17,tweets_polo_16,tweets_polo_15,tweets_polo_14,tweets_polo_13)
tweets_pe <-rbind(tweets_pe_21,tweets_pe_20,tweets_pe_19,tweets_pe_18,tweets_pe_17,tweets_pe_16,tweets_pe_15,tweets_pe_14,tweets_pe_13)

```

# Performing transformations

## cleaning data (text)
```{r}
#Separate date column
News24_tweets <- separate(News24_tweets, created_at, into=c("date","time"), sep=" ") %>% mutate(date = as.Date(date)) %>% select(date, time, text, source, favorite_count, retweet_count)
TimesLIVE_tweets <- separate(TimesLIVE_tweets, created_at, into=c("date","time"), sep=" ") %>% mutate(date = as.Date(date)) %>% select(date, time, text, source, favorite_count, retweet_count)
ewnupdates_tweets <- separate(ewnupdates_tweets, created_at, into=c("date","time"), sep=" ") %>% mutate(date = as.Date(date)) %>% select(date, time, text, source, favorite_count, retweet_count)
IOL_tweets <- separate(IOL_tweets, created_at, into=c("date","time"), sep=" ") %>% mutate(date = as.Date(date)) %>% select(date, time, text, source, favorite_count, retweet_count)
eNCA_tweets <- separate(eNCA_tweets, created_at, into=c("date","time"), sep=" ") %>% mutate(date = as.Date(date)) %>% select(date, time, text, source, favorite_count, retweet_count)
SABCNews_tweets <- separate(SABCNews_tweets, created_at, into=c("date","time"), sep=" ") %>% mutate(date = as.Date(date)) %>% select(date, time, text, source, favorite_count, retweet_count)


#Subset data into 3 week range 1 June - 21 June

myfuncNews24 <- function(x,y){News24_tweets[News24_tweets$date >= x & News24_tweets$date <= y,]}
myfunctimesLIVE <- function(x,y){TimesLIVE_tweets[TimesLIVE_tweets$date >= x & TimesLIVE_tweets$date <= y,]}
myfuncEwnupdate <- function(x,y){ewnupdates_tweets[ewnupdates_tweets$date >= x & ewnupdates_tweets$date <= y,]}
myfuncIOL <- function(x,y){IOL_tweets[IOL_tweets$date >= x & IOL_tweets$date <= y,]}
myfunceNCA <- function(x,y){eNCA_tweets[eNCA_tweets$date >= x & eNCA_tweets$date <= y,]}
myfuncSABC <- function(x,y){SABCNews_tweets[SABCNews_tweets$date >= x & SABCNews_tweets$date <= y,]}

DATE1 <- as.Date("2020-06-01")
DATE2 <- as.Date("2020-06-21")

News24_tweets <- myfuncNews24(DATE1,DATE2)
TimesLIVE_tweets <- myfunctimesLIVE(DATE1,DATE2)
ewnupdates_tweets <- myfuncEwnupdate(DATE1,DATE2)
IOL_tweets <- myfuncIOL(DATE1,DATE2)
eNCA_tweets <- myfunceNCA(DATE1,DATE2)
SABCNews_tweets <- myfuncSABC(DATE1,DATE2)


```

# AddItional cleaning per dataframe
```{r}
#News24 cleaning
tweets.News24 <- News24_tweets %>% select(text)

tweets.News24_clean <- unique(tweets.News24)
tweets.News24_clean$text <- iconv(tweets.News24_clean$text,"UTF-8", "ASCII", sub="")#to remove emoji's 
tweets.News24_clean$stripped_text <- tolower(tweets.News24_clean$text)


tweets.News24_clean$stripped_text <- gsub("http[^[:space:]]*", " ", tweets.News24_clean$stripped_text) # remove links
tweets.News24_clean$stripped_text <- gsub("@\\S*","", tweets.News24_clean$stripped_text) # remove mentions
tweets.News24_clean$stripped_text <- gsub("amp","", tweets.News24_clean$stripped_text) 
tweets.News24_clean$stripped_text <- gsub("[\r\n]","", tweets.News24_clean$stripped_text)
tweets.News24_clean$stripped_text <- gsub("[[:punct:]]"," ", tweets.News24_clean$stripped_text) #remove punctuation
tweets.News24_clean$stripped_text = gsub("[ |\t]{2,}", " ", tweets.News24_clean$stripped_text) # remove tabs
tweets.News24_clean$stripped_text = gsub("^ ", "", tweets.News24_clean$stripped_text) # leading blanks
tweets.News24_clean$stripped_text = gsub(" $", "", tweets.News24_clean$stripped_text) #lagging blanks
tweets.News24_clean$stripped_text = gsub(" +", " ", tweets.News24_clean$stripped_text) #general spaces


tweets.News24_clean <- tweets.News24_clean %>% select(stripped_text) 
tweets.News24_stem <- tweets.News24_clean %>% unnest_tokens(word, stripped_text)

clean_tweets.News24 <- tweets.News24_stem %>% anti_join(stop_words)

#clean_tweets.News24
```


```{r}
#TimesLIVE cleaning
tweets.TimesLIVE <- TimesLIVE_tweets %>% select(text)

tweets.TimesLIVE_clean <- unique(tweets.TimesLIVE)
tweets.TimesLIVE_clean$text <- iconv(tweets.TimesLIVE_clean$text,"UTF-8", "ASCII", sub="")#to remove emoji's 
tweets.TimesLIVE_clean$stripped_text <- tolower(tweets.TimesLIVE_clean$text)


tweets.TimesLIVE_clean$stripped_text <- gsub("http.+ |http.+$", " ", tweets.TimesLIVE_clean$stripped_text) # remove links
tweets.TimesLIVE_clean$stripped_text <- gsub("@\\S*","", tweets.TimesLIVE_clean$stripped_text) # remove mentions
tweets.TimesLIVE_clean$stripped_text <- gsub("amp","", tweets.TimesLIVE_clean$stripped_text) 
tweets.TimesLIVE_clean$stripped_text <- gsub("[\r\n]","", tweets.TimesLIVE_clean$stripped_text)
tweets.TimesLIVE_clean$stripped_text <- gsub("[[:punct:]]"," ", tweets.TimesLIVE_clean$stripped_text) #remove punctuation
tweets.TimesLIVE_clean$stripped_text = gsub("[ |\t]{2,}", " ", tweets.TimesLIVE_clean$stripped_text) # remove tabs
tweets.TimesLIVE_clean$stripped_text = gsub("^ ", "", tweets.TimesLIVE_clean$stripped_text) # leading blanks
tweets.TimesLIVE_clean$stripped_text = gsub(" $", "", tweets.TimesLIVE_clean$stripped_text) #lagging blanks
tweets.TimesLIVE_clean$stripped_text = gsub(" +", " ", tweets.TimesLIVE_clean$stripped_text) #general spaces


tweets.TimesLIVE_clean <- tweets.TimesLIVE_clean %>% select(stripped_text)
tweets.TimesLIVE_stem <- tweets.TimesLIVE_clean  %>% unnest_tokens(word, stripped_text)

clean_tweets.TimesLIVE <- tweets.TimesLIVE_stem %>% anti_join(stop_words)

#clean_tweets.TimesLIVE
```

```{r}
#eNCA cleaning
tweets.eNCA <- eNCA_tweets %>% select(text)

tweets.eNCA_clean <- unique(tweets.eNCA)
tweets.eNCA_clean$text <- iconv(tweets.eNCA_clean$text,"UTF-8", "ASCII", sub="")#to remove emoji's 
tweets.eNCA_clean$stripped_text <- tolower(tweets.eNCA_clean$text)


tweets.eNCA_clean$stripped_text <- gsub("http.+ |http.+$", " ", tweets.eNCA_clean$stripped_text) # remove links
tweets.eNCA_clean$stripped_text <- gsub("@\\S*","", tweets.eNCA_clean$stripped_text) # remove mentions
tweets.eNCA_clean$stripped_text <- gsub("amp","", tweets.eNCA_clean$stripped_text) 
tweets.eNCA_clean$stripped_text <- gsub("[\r\n]","", tweets.eNCA_clean$stripped_text)
tweets.eNCA_clean$stripped_text <- gsub("[[:punct:]]"," ", tweets.eNCA_clean$stripped_text) #remove punctuation
tweets.eNCA_clean$stripped_text = gsub("[ |\t]{2,}", " ", tweets.eNCA_clean$stripped_text) # remove tabs
tweets.eNCA_clean$stripped_text = gsub("^ ", "", tweets.eNCA_clean$stripped_text) # leading blanks
tweets.eNCA_clean$stripped_text = gsub(" $", "", tweets.eNCA_clean$stripped_text) #lagging blanks
tweets.eNCA_clean$stripped_text = gsub(" +", " ", tweets.eNCA_clean$stripped_text) #general spaces

tweets.eNCA_clean <- tweets.eNCA_clean %>% select(stripped_text)
tweets.eNCA_stem <- tweets.eNCA_clean %>% unnest_tokens(word, stripped_text)

clean_tweets.eNCA <- tweets.eNCA_stem %>% anti_join(stop_words)

#clean_tweets.eNCA
```


```{r}
#IOL cleaning
tweets.IOL <- IOL_tweets %>% select(text)

tweets.IOL_clean <- unique(tweets.IOL)
tweets.IOL_clean$text <- iconv(tweets.IOL_clean$text,"UTF-8", "ASCII", sub="")#to remove emoji's 

tweets.IOL_clean$stripped_text <- tolower(tweets.IOL_clean$text)


tweets.IOL_clean$stripped_text <- gsub("http.+ |http.+$", " ", tweets.IOL_clean$stripped_text) # remove links
tweets.IOL_clean$stripped_text <- gsub("@\\S*","", tweets.IOL_clean$stripped_text) # remove mentions
tweets.IOL_clean$stripped_text <- gsub("amp","", tweets.IOL_clean$stripped_text) 
tweets.IOL_clean$stripped_text <- gsub("[\r\n]","", tweets.IOL_clean$stripped_text)
tweets.IOL_clean$stripped_text <- gsub("[[:punct:]]"," ", tweets.IOL_clean$stripped_text) #remove punctuation
tweets.IOL_clean$stripped_text = gsub("[ |\t]{2,}", " ", tweets.IOL_clean$stripped_text) # remove tabs
tweets.IOL_clean$stripped_text = gsub("^ ", "", tweets.IOL_clean$stripped_text) # leading blanks
tweets.IOL_clean$stripped_text = gsub(" $", "", tweets.IOL_clean$stripped_text) #lagging blanks
tweets.IOL_clean$stripped_text = gsub(" +", " ", tweets.IOL_clean$stripped_text) #general spaces

tweets.IOL_clean <- tweets.IOL_clean %>% select(stripped_text)
tweets.IOL_stem <- tweets.IOL_clean %>% unnest_tokens(word, stripped_text)

clean_tweets.IOL <- tweets.IOL_stem %>% anti_join(stop_words)

#clean_tweets.IOL
```

```{r}
#ewn cleaning
tweets.ewn <- ewnupdates_tweets %>% select(text)

tweets.ewn_clean <- unique(tweets.ewn)
tweets.ewn_clean$text <- iconv(tweets.ewn_clean$text,"UTF-8", "ASCII", sub="")#to remove emojis 
tweets.ewn_clean$stripped_text <- tolower(tweets.ewn_clean$text)


tweets.ewn_clean$stripped_text <- gsub("http.+ |http.+$", " ", tweets.ewn_clean$stripped_text) # remove links
tweets.ewn_clean$stripped_text <- gsub("@\\S*","", tweets.ewn_clean$stripped_text) # remove mentions
tweets.ewn_clean$stripped_text <- gsub("amp","", tweets.ewn_clean$stripped_text) 
tweets.ewn_clean$stripped_text <- gsub("[\r\n]","", tweets.ewn_clean$stripped_text)
tweets.ewn_clean$stripped_text <- gsub("[[:punct:]]"," ", tweets.ewn_clean$stripped_text) #remove punctuation
tweets.ewn_clean$stripped_text = gsub("[ |\t]{2,}", " ", tweets.ewn_clean$stripped_text) # remove tabs
tweets.ewn_clean$stripped_text = gsub("^ ", "", tweets.ewn_clean$stripped_text) # leading blanks
tweets.ewn_clean$stripped_text = gsub(" $", "", tweets.ewn_clean$stripped_text) #lagging blanks
tweets.ewn_clean$stripped_text = gsub(" +", " ", tweets.ewn_clean$stripped_text) #general spaces


tweets.ewn_clean <- tweets.ewn_clean %>% select(stripped_text)
tweets.ewn_stem <- tweets.ewn_clean  %>% unnest_tokens(word, stripped_text)

clean_tweets.ewn <- tweets.ewn_stem %>% anti_join(stop_words)

#clean_tweets.ewn
```

```{r}
#SABCNews cleaning
tweets.SABCNews <- SABCNews_tweets %>% select(text)

tweets.SABCNews_clean <- unique(tweets.SABCNews)
tweets.SABCNews_clean$text <- iconv(tweets.SABCNews_clean$text,"UTF-8", "ASCII", sub="")#to remove emoji's 
tweets.SABCNews_clean$stripped_text <- tolower(tweets.SABCNews_clean$text)


tweets.SABCNews_clean$stripped_text <- gsub("http.+ |http.+$", " ", tweets.SABCNews_clean$stripped_text) # remove links
tweets.SABCNews_clean$stripped_text <- gsub("@\\S*","", tweets.SABCNews_clean$stripped_text) # remove mentions
tweets.SABCNews_clean$stripped_text <- gsub("amp","", tweets.SABCNews_clean$stripped_text) 
tweets.SABCNews_clean$stripped_text <- gsub("[\r\n]","", tweets.SABCNews_clean$stripped_text)
tweets.SABCNews_clean$stripped_text <- gsub("[[:punct:]]"," ", tweets.SABCNews_clean$stripped_text) #remove punctuation
tweets.SABCNews_clean$stripped_text = gsub("[ |\t]{2,}", " ", tweets.SABCNews_clean$stripped_text) # remove tabs
tweets.SABCNews_clean$stripped_text = gsub("^ ", "", tweets.SABCNews_clean$stripped_text) # leading blanks
tweets.SABCNews_clean$stripped_text = gsub(" $", "", tweets.SABCNews_clean$stripped_text) #lagging blanks
tweets.SABCNews_clean$stripped_text = gsub(" +", " ", tweets.SABCNews_clean$stripped_text) #general spaces

tweets.SABCNews_clean <- tweets.SABCNews_clean %>% select(stripped_text)
tweets.SABCNews_stem <- tweets.SABCNews_clean %>% unnest_tokens(word, stripped_text)

clean_tweets.SABCNews <- tweets.SABCNews_stem  %>% anti_join(stop_words)

#clean_tweets.SABCNews
```

```{r}
#Global Data
global_cleaned <- rbind(tweets.SABCNews_clean,tweets.ewn_clean,tweets.IOL_clean,tweets.eNCA_clean,tweets.TimesLIVE_clean,tweets.News24_clean)

global_tweets = rbind(SABCNews_tweets,eNCA_tweets,IOL_tweets,ewnupdates_tweets,TimesLIVE_tweets,News24_tweets)

```


```{r}
# AFFIN Sentiment Analysis
# this example uses 

#bing_news24 <- clean_tweets.News24 %>% inner_join(get_sentiments("bing")) %>% count(word, sentiment, sort = TRUE) %>% ungroup()

#bing_news24

afinn_news24 <- clean_tweets.News24 %>% inner_join(get_sentiments("afinn")) %>% count(word, value, sort = TRUE) %>% ungroup()
afinn_news24

afinn_TimesLIVE <- clean_tweets.TimesLIVE %>% inner_join(get_sentiments("afinn")) %>% count(word, value, sort = TRUE) %>% ungroup()
afinn_TimesLIVE

afinn_eNCA <- clean_tweets.eNCA %>% inner_join(get_sentiments("afinn")) %>% count(word, value, sort = TRUE) %>% ungroup()
afinn_eNCA

afinn_IOL <- clean_tweets.IOL %>% inner_join(get_sentiments("afinn")) %>% count(word, value, sort = TRUE) %>% ungroup()
afinn_IOL

afinn_ewn <- clean_tweets.ewn %>% inner_join(get_sentiments("afinn")) %>% count(word, value, sort = TRUE) %>% ungroup()
afinn_ewn

afinn_SABCNews <- clean_tweets.SABCNews %>% inner_join(get_sentiments("afinn")) %>% count(word, value, sort = TRUE) %>% ungroup()
afinn_SABCNews

```

# Topic Analysis
## Individual Topic Analysis
### LDA Topic Analysis
```{r}
# topic modeling using LDA
# example on rpubs.com

corpusNews24 <- Corpus(VectorSource(tweets.News24_clean))
corpusNews24 <- tm_map(corpusNews24, removeWords, stopwords("english"))  
corpusNews24 <- tm_map(corpusNews24, removeNumbers)
corpusNews24 <- tm_map(corpusNews24, stemDocument)
#Remove search terms -->
dtmNews24 = DocumentTermMatrix(corpusNews24)

corpusTimesLIVE <- Corpus(VectorSource(clean_tweets.TimesLIVE))
corpusTimesLIVE <- tm_map(corpusTimesLIVE, removeWords, stopwords("english"))  
corpusTimesLIVE <- tm_map(corpusTimesLIVE, removeNumbers)
corpusTimesLIVE <- tm_map(corpusTimesLIVE, stemDocument)
dtmTimesLIVE = DocumentTermMatrix(corpusTimesLIVE)

corpuseNCA <- Corpus(VectorSource(clean_tweets.eNCA))
corpuseNCA <- tm_map(corpuseNCA, removeWords, stopwords("english"))  
corpuseNCA <- tm_map(corpuseNCA, removeNumbers)
corpuseNCA <- tm_map(corpuseNCA, stemDocument)
dtmeNCA = DocumentTermMatrix(corpuseNCA)

corpusIOL <- Corpus(VectorSource(clean_tweets.IOL))
corpusIOL <- tm_map(corpusIOL, removeWords, stopwords("english"))  
corpusIOL <- tm_map(corpusIOL, removeNumbers)
corpusIOL <- tm_map(corpusIOL, stemDocument)
dtmIOL = DocumentTermMatrix(corpusIOL)

corpusewn <- Corpus(VectorSource(clean_tweets.ewn))
corpusewn <- tm_map(corpusewn, removeWords, stopwords("english"))  
corpusewn <- tm_map(corpusewn, removeNumbers)
corpusewn <- tm_map(corpusewn, stemDocument)
dtmewn = DocumentTermMatrix(corpusewn)

corpusSABCNews <- Corpus(VectorSource(clean_tweets.SABCNews))
corpusSABCNews <- tm_map(corpusSABCNews, removeWords, stopwords("english"))  
corpusSABCNews <- tm_map(corpusSABCNews, removeNumbers)
corpusSABCNews <- tm_map(corpusSABCNews, stemDocument)
dtmSABCNews = DocumentTermMatrix(corpusSABCNews)


```

#####################################################################################################################
Remember to delete chunk
```{r}
#News24 LDA topic analysis

#LDA model with 2 topics selected
News24_lda_2 = LDA(dtmNews24, k = 2, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#LDA model with 5 topics selected
News24_lda_5 = LDA(dtmNews24, k = 5, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#LDA model with 10 topics selected
News24_lda_10 = LDA(dtmNews24, k = 10, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#News24_lda_20 = LDA(dtmNews24, k = 20, method = 'Gibbs', 
#          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
#                         thin = 500, burnin = 4000, iter = 2000))


News24_lda = LDA(dtmNews24, k = 10, method = t, control = list(seed = 1234))
News24_lda = LDA(dtmNews24, k = 10, control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))
# comparing total tweets by medias vs the amount that was related to COVID-19 COULD THIS BE SEEN AS TOPIC ANALYSIS???????

```
#####################################################################################################################

```{r}
#LDA model with 10 topics selected
#News24_lda = LDA(dtmNews24, k = 10, method = 'Gibbs', control = list(seed = 1234))
News24_lda = LDA(dtmNews24, k = 10, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))


#LDA model with 10 topics selected
TimesLIVE_lda = LDA(dtmTimesLIVE, k = 10, method = 'Gibbs', control = list(seed = 1234))
TimesLIVE_lda = LDA(dtmTimesLIVE, k = 10, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#LDA model with 10 topics selected
eNCA_lda = LDA(dtmeNCA, k = 10, method = 'Gibbs', control = list(seed = 1234))

#LDA model with 10 topics selected
IOL_lda = LDA(dtmIOL, k = 10, method = 'Gibbs', control = list(seed = 1234))

#LDA model with 10 topics selected
ewn_lda = LDA(dtmewn, k = 10, method = 'Gibbs', control = list(seed = 1234))

#LDA model with 10 topics selected
SABCNews_lda = LDA(dtmSABCNews, k = 10, method = 'Gibbs', control = list(seed = 1234))
```


```{r}
#Top 5 terms or words under each topic
top5terms_News24 = as.matrix(terms(News24_lda,5))
top5terms_TimesLIVE = as.matrix(terms(TimesLIVE_lda,5))
top5terms_eNCA = as.matrix(terms(eNCA_lda,5))
top5terms_IOL = as.matrix(terms(IOL_lda,5))
top5terms_ewn = as.matrix(terms(ewn_lda,5))
top5terms_SABCNews = as.matrix(terms(SABCNews_lda,5))


top5terms_News24
top5terms_TimesLIVE
top5terms_eNCA
top5terms_IOL
top5terms_ewn
top5terms_SABCNews

```

```{r}
News24_topics_LDA <- tidy(News24_lda, matrix = "beta")
TimesLIVE_topics_LDA <- tidy(TimesLIVE_lda, matrix = "beta")
eNCA_topics_LDA <- tidy(eNCA_lda, matrix = "beta")
IOL_topics_LDA <- tidy(IOL_lda, matrix = "beta")
ewn_topics_LDA <- tidy(ewn_lda, matrix = "beta")
SABCNews_topics_LDA <- tidy(SABCNews_lda, matrix = "beta")

News24_top_terms_LDA <- 
  News24_topics_LDA %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

TimesLIVE_top_terms_LDA <- 
  TimesLIVE_topics_LDA %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

eNCA_top_terms_LDA <- 
  eNCA_topics_LDA %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

IOL_top_terms_LDA <- 
  IOL_topics_LDA %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ewn_top_terms_LDA <- 
  ewn_topics_LDA %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

SABCNews_top_terms_LDA <- 
  SABCNews_topics_LDA %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

```


```{r}
#Topics found out by our model:
lda.topics_News24 = as.matrix(topics(News24_lda))
lda.topics_TimesLIVE = as.matrix(topics(TimesLIVE_lda))
lda.topics_eNCA = as.matrix(topics(eNCA_lda))
lda.topics_IOL = as.matrix(topics(IOL_lda))
lda.topics_ewn = as.matrix(topics(ewn_lda))
lda.topics_SABCNews = as.matrix(topics(SABCNews_lda))


summary(as.factor(lda.topics_News24[,1]))
summary(as.factor(lda.topics_TimesLIVE[,1]))
summary(as.factor(lda.topics_eNCA[,1]))
summary(as.factor(lda.topics_IOL[,1]))
summary(as.factor(lda.topics_ewn[,1]))
summary(as.factor(lda.topics_SABCNews[,1]))
```


```{r}
#document wise probability of each topic.
topicprob_News24 = as.matrix(News24_lda@gamma)
topicprob_TimesLIVE = as.matrix(TimesLIVE_lda@gamma)
topicprob_eNCA = as.matrix(eNCA_lda@gamma)
topicprob_IOL = as.matrix(IOL_lda@gamma)
topicprob_ewn = as.matrix(ewn_lda@gamma)
topicprob_SABCNews = as.matrix(SABCNews_lda@gamma)


head(topicprob_News24,1)
head(topicprob_TimesLIVE,1)
head(topicprob_eNCA,1)
head(topicprob_IOL,1)
head(topicprob_ewn,1)
head(topicprob_SABCNews,1)
```

### STM Topic Modelling
```{r}
processed_News24 <- textProcessor(tweets.News24_clean$stripped_text, metadata = News24_tweets)
processed_TimesLIVE <- textProcessor(tweets.TimesLIVE_clean$stripped_text, metadata = TimesLIVE_tweets)
processed_ewn <- textProcessor(tweets.ewn_clean$stripped_text, metadata = ewnupdates_tweets)
processed_IOL <- textProcessor(tweets.IOL_clean$stripped_text, metadata = IOL_tweets)
processed_eNCA <- textProcessor(tweets.eNCA_clean$stripped_text, metadata = eNCA_tweets)
processed_SABC <- textProcessor(tweets.SABCNews_clean$stripped_text, metadata = SABCNews_tweets)
processed_global <- textProcessor(tweets.SABCNews_clean$stripped_text, metadata = global_tweets)

```

```{r}
out_News24 <- prepDocuments(processed_News24$documents, processed_News24$vocab, processed_News24$meta)
out_TimesLIVE <- prepDocuments(processed_TimesLIVE$documents, processed_TimesLIVE$vocab, processed_TimesLIVE$meta)
out_ewn <- prepDocuments(processed_ewn$documents, processed_ewn$vocab, processed_ewn$meta)
out_IOL <- prepDocuments(processed_IOL$documents, processed_IOL$vocab, processed_IOL$meta)
out_eNCA <- prepDocuments(processed_eNCA$documents, processed_eNCA$vocab, processed_eNCA$meta)
out_SABC <- prepDocuments(processed_SABC$documents, processed_SABC$vocab, processed_SABC$meta)
out_global <- prepDocuments(processed_global$documents, processed_global$vocab, processed_global$meta)
```

```{r}
#News24
News24_docs <- out_News24$documents
News24_vocab <- out_News24$vocab
News24_meta <-out_News24$meta

#TimesLIVE
TimesLIVE_docs <- out_TimesLIVE$documents
TimesLIVE_vocab <- out_TimesLIVE$vocab
TimesLIVE_meta <-out_TimesLIVE$meta

#EWN
ewn_docs <- out_ewn$documents
ewn_vocab <- out_ewn$vocab
ewn_meta <-out_ewn$meta

#IOL
IOL_docs <- out_IOL$documents
IOL_vocab <- out_IOL$vocab
IOL_meta <-out_IOL$meta

#eNCA
eNCA_docs <- out_eNCA$documents
eNCA_vocab <- out_eNCA$vocab
eNCA_meta <-out_eNCA$meta

#SABC
SABC_docs <- out_SABC$documents
SABC_vocab <- out_SABC$vocab
SABC_meta <-out_SABC$meta

#Global
global_docs <- out_global$documents
global_vocab <- out_global$vocab
global_meta <-out_global$meta
```

#Delete after final K has been found
#####################################################################################################################
```{r}
First_STM <- stm(documents = docs, vocab = vocab,
              K = 10, max.em.its = 75, data = meta,
              init.type = "Spectral", verbose = FALSE)

Second_STM <- stm(documents = out$documents, vocab = out$vocab,
              K = 15, max.em.its = 75, data = out$meta,
              init.type = "Spectral", verbose = FALSE)

Third_STM <- stm(documents = out$documents, vocab = out$vocab,
              K = 20, max.em.its = 75, data = out$meta,
              init.type = "Spectral", verbose = FALSE)
```


```{r}
plot(First_STM)
plot(Second_STM)
plot(Third_STM)
```

```{r}
global_findingk <- searchK(documents = global_docs, vocab = global_vocab, K = c(5:50),
 prevalence =~ date, data = global_meta, verbose=FALSE)
```

```{r}
global_findingk <- read_rds("Data/Final_Data/global_findingk.rds")
write_rds(global_findingk, "Data/Final_Data/global_findingk.rds")
```

```{r}
#load("./findingk.Rda")

plot(global_findingk)
```

#####################################################################################################################



```{r}
#Running STM per news agency

News24_STM <- stm(documents = News24_docs, vocab = News24_vocab,
              K = 10, max.em.its = 75, data = News24_meta,
              init.type = "Spectral", verbose = FALSE)

timesLIVE_STM <- stm(documents = TimesLIVE_docs, vocab = TimesLIVE_vocab,
              K = 10, max.em.its = 75, data = TimesLIVE_meta,
              init.type = "Spectral", verbose = FALSE)

Ewnupdate_STM <- stm(documents = ewn_docs, vocab = ewn_vocab,
              K = 10, max.em.its = 75, data = ewn_meta,
              init.type = "Spectral", verbose = FALSE)

IOL_STM <- stm(documents = IOL_docs, vocab = IOL_vocab,
              K = 10, max.em.its = 75, data = IOL_meta,
              init.type = "Spectral", verbose = FALSE)

eNCA_STM <- stm(documents = eNCA_docs, vocab = eNCA_vocab,
              K = 10, max.em.its = 75, data = eNCA_meta,
              init.type = "Spectral", verbose = FALSE)

SABC_STM <- stm(documents = SABC_docs, vocab = SABC_vocab,
              K = 10, max.em.its = 75, data = SABC_meta,
              init.type = "Spectral", verbose = FALSE)

global_STM <- stm(documents = global_docs, vocab = global_vocab,
              K = 10, max.em.its = 75, data = global_meta,
              init.type = "Spectral", verbose = FALSE)

```


```{r}
News24_topics <- tidy(News24_STM, matrix = "beta")
timesLIVE_topics <- tidy(timesLIVE_STM, matrix = "beta")
Ewnupdate_topics <- tidy(Ewnupdate_STM, matrix = "beta")
IOL_topics <- tidy(IOL_STM, matrix = "beta")
eNCA_topics <- tidy(eNCA_STM, matrix = "beta")
SABC_topics <- tidy(SABC_STM, matrix = "beta")
global_topics <- tidy(global_STM, matrix = "beta")

News24_top_terms <- 
  News24_topics %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

timesLIVE_top_terms <- 
  timesLIVE_topics %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

Ewnupdate_top_terms <- 
  Ewnupdate_topics %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

IOL_top_terms <- 
  IOL_topics %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

eNCA_top_terms <- 
  eNCA_topics %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

SABC_top_terms <- 
  SABC_topics %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

global_top_terms <- 
  global_topics %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

```



```{r}
#LDA
write_rds(News24_top_terms_LDA, "Data/Final_Data/News24_top_terms_LDA.rds")
write_rds(TimesLIVE_top_terms_LDA, "Data/Final_Data/timesLIVE_top_terms_LDA.rds")
write_rds(ewn_top_terms_LDA, "Data/Final_Data/Ewnupdate_top_terms_LDA.rds")
write_rds(IOL_top_terms_LDA, "Data/Final_Data/IOL_top_terms_LDA.rds")
write_rds(eNCA_top_terms_LDA, "Data/Final_Data/eNCA_top_terms_LDA.rds")
write_rds(SABCNews_top_terms_LDA, "Data/Final_Data/SABC_top_terms_LDA.rds")
#write_rds(global_top_terms_LDA, "Data/Final_Data/global_top_terms_LDA.rds")

#STM
write_rds(News24_top_terms, "Data/Final_Data/News24_top_terms.rds")
write_rds(timesLIVE_top_terms, "Data/Final_Data/timesLIVE_top_terms.rds")
write_rds(Ewnupdate_top_terms, "Data/Final_Data/Ewnupdate_top_terms.rds")
write_rds(IOL_top_terms, "Data/Final_Data/IOL_top_terms.rds")
write_rds(eNCA_top_terms, "Data/Final_Data/eNCA_top_terms.rds")
write_rds(SABC_top_terms, "Data/Final_Data/SABC_top_terms.rds")
write_rds(global_top_terms, "Data/Final_Data/global_top_terms.rds")
```

```{r}
# theme
Other_theme <- theme(
  axis.text.x=element_text(colour = "black",face = "bold"),
  axis.text.y=element_text(colour = "black",face = "bold"),
  axis.title.x=element_blank(),
  axis.title.y=element_blank(),
  panel.border=element_blank(),
  panel.grid.major=element_line(colour = "black"), 
  panel.grid.minor=element_blank(),
  plot.background=element_rect(fill = "grey"),
  plot.caption = element_text(colour ="black",face = "bold" ,hjust = +1),
  strip.text = element_text(face = "bold"),
  plot.title = element_text(size = 20, colour ="black",face = "bold" ,hjust = 0.5),
)

```



# Other Analysis
```{r}
# 3 week period
# Frequency of tweets by day
 #%>% group_by(date)
News24_freq <- News24_tweets %>% group_by(date) %>% summarise(News24 = n())
timesLIVE_freq <- timesLIVE_tweets %>% group_by(date) %>% summarise(timesLIVE = n()) %>% select(timesLIVE)
Ewnupdate_freq <- Ewnupdate_tweets %>% group_by(date) %>% summarise(Ewnupdate = n()) %>% select(Ewnupdate)
IOL_freq <- IOL_tweets %>% group_by(date) %>% summarise(IOL = n()) %>% select(IOL)
eNCA_freq <- eNCA_tweets %>% group_by(date) %>% summarise(eNCA = n()) %>% select(eNCA)
SABC_freq <- SABC_tweets %>% group_by(date) %>% summarise(SABC = n()) %>% select(SABC)

# sum days and join together, then send to visualisation
Frequency_tweets <- bind_cols(News24_freq,timesLIVE_freq,Ewnupdate_freq,IOL_freq,eNCA_freq,SABC_freq)

ggplot(Frequency_tweets, aes(x = date, y = News24)) + Other_theme + geom_line(color = "grey", size = 0.9) + geom_line(aes(x = date, y = timesLIVE), color = "blue", size = 0.9) + geom_line(aes(x = date, y = Ewnupdate), color = "red", size = 0.9) + geom_line(aes(x = date, y = IOL), color = "brown", size = 0.9) + geom_line(aes(x = date, y = eNCA), color = "green", size = 0.9) + geom_line(aes(x = date, y = SABC), color = "yellow", size = 0.9) + scale_x_date(breaks = "1 day",date_labels = "%d") + scale_y_continuous(breaks = seq(0, 160, by = 20)) + labs(x = NULL, y = NULL, title = "Frequency of Tweets", subtitle = "Three week period in June", caption = "\nSource: Data collected from Twitter's REST API via rtweet")


#retweet count 
News24_retweet <- News24_tweets %>% group_by(date) %>% summarise(News24 = sum(retweet_count)) 
timesLIVE_retweet <- timesLIVE_tweets %>% group_by(date) %>% summarise(timesLIVE = sum(retweet_count)) %>% select(timesLIVE)
Ewnupdate_retweet <- Ewnupdate_tweets %>% group_by(date) %>% summarise(Ewnupdate = sum(retweet_count)) %>% select(Ewnupdate)
IOL_retweet <- IOL_tweets %>% group_by(date) %>% summarise(IOL = sum(retweet_count)) %>% select(IOL)
eNCA_retweet <- eNCA_tweets %>% group_by(date) %>% summarise(eNCA = sum(retweet_count)) %>% select(eNCA)
SABC_retweet <- SABC_tweets %>% group_by(date) %>% summarise(SABC = sum(retweet_count)) %>% select(SABC)

# sum days and join together, then send to visualisation
retweet_tweets <- bind_cols(News24_retweet,timesLIVE_retweet,Ewnupdate_retweet,IOL_retweet,eNCA_retweet,SABC_retweet)

ggplot(retweet_tweets, aes(x = date, y = News24)) + Other_theme + geom_line(color = "grey", size = 0.9) + geom_line(aes(x = date, y = timesLIVE), color = "blue", size = 0.9) + geom_line(aes(x = date, y = Ewnupdate), color = "red", size = 0.9) + geom_line(aes(x = date, y = IOL), color = "brown", size = 0.9) + geom_line(aes(x = date, y = eNCA), color = "green", size = 0.9) + geom_line(aes(x = date, y = SABC), color = "yellow", size = 0.9) + scale_x_date(breaks = "1 day",date_labels = "%d")+ scale_y_continuous(trans = "log2", breaks = c(250,500,750,1000,2000,4000,6000,8000)) + labs(x = NULL, y = NULL, title = "Retweet count", subtitle = "Three week period in June", caption = "\nSource: Data collected from Twitter's REST API via rtweet")

# like count
News24_likes <- News24_tweets %>% group_by(date) %>% summarise(News24 = sum(favorite_count)) 
timesLIVE_likes <- timesLIVE_tweets %>% group_by(date) %>% summarise(timesLIVE = sum(favorite_count)) %>% select(timesLIVE) 
Ewnupdate_likes <- Ewnupdate_tweets %>% group_by(date) %>% summarise(Ewnupdate = sum(favorite_count)) %>% select(Ewnupdate)
IOL_likes <- IOL_tweets %>% group_by(date) %>% summarise(IOL = sum(favorite_count)) %>% select(IOL)
eNCA_likes <- eNCA_tweets %>% group_by(date) %>% summarise(eNCA = sum(favorite_count)) %>% select(eNCA)
SABC_likes <- SABC_tweets %>% group_by(date) %>% summarise(SABC = sum(favorite_count)) %>% select(SABC)

# sum days and join together, then send to visualisation
Likes_tweets <- bind_cols(News24_likes,timesLIVE_likes,Ewnupdate_likes,IOL_likes,eNCA_likes,SABC_likes)

ggplot(Likes_tweets, aes(x = date, y = News24)) + Other_theme + geom_line(color = "grey", size = 0.9) + geom_line(aes(x = date, y = timesLIVE), color = "blue", size = 0.9) + geom_line(aes(x = date, y = Ewnupdate), color = "red", size = 0.9) + geom_line(aes(x = date, y = IOL), color = "brown", size = 0.9) + geom_line(aes(x = date, y = eNCA), color = "green", size = 0.9) + geom_line(aes(x = date, y = SABC), color = "yellow", size = 0.9) + scale_x_date(breaks = "1 day",date_labels = "%d") + scale_y_continuous(breaks = seq(0, 20000, by = 2500)) + labs(x = NULL, y = NULL, title = "Like counter", subtitle = "Three week period in June", caption = "\nSource: Data collected from Twitter's REST API via rtweet")



```

```{r}
#count the total number of tweets for each media house
News24_freq <- nrow(News24_freq)
timesLIVE_freq <- nrow(timesLIVE_freq)
Ewnupdate_freq <- nrow(Ewnupdate_freq)
IOL_freq <- nrow(IOL_freq)
eNCA_freq <- nrow(eNCA_freq)
SABC_freq <- nrow(SABC_freq)

News24_freq
timesLIVE_freq
Ewnupdate_freq
IOL_freq
eNCA_freq
SABC_freq
# Comparing stats of non-covid to covid
# count row of tweets and compare to nrow in database that will be filtered tweets for covid relation
#start
#end
```


```{r}

```


## Done

# Exporting transformation results
```{r echo=TRUE}
#write_rds(News24_tweets, "Results/News24_tweets.rds")
#write_rds(TimesLIVE_tweets, "Results/TimesLIVE_tweets.rds")
#write_rds(eNCA_tweets, "Results/eNCA_tweets.rds")
#write_rds(IOL_tweets,"Results/IOL_tweets.rds")
#write_rds(SABCNews_tweets,"Results/SABCNews_tweets.rds")
#write_rds(ewnupdates_tweets,"Results/ewnupdates_tweets")
```

```{r}




```



```

## Exporting results
```{r}

```