---
title: "Data Transformation"
output: html_notebook
---

```{r}
#install_github("mkearney/rtweet")

#Packages
library(usethis)
library(devtools)
library(dplyr)
library(rtweet)
library(readr)
library(topicmodels)
library(tidyverse)
library(maps)
library(tidytext)
library(textdata)
library(tm)
```

## Load extracted data 
```{r}
News24_tweets <- read_rds("Data/Raw_Timeline_Data/News24_tweets.rds")
TimesLIVE_tweets <- read_rds("Data/Raw_Timeline_Data/TimesLIVE_tweets.rds")
eNCA_tweets <- read_rds("Data/Raw_Timeline_Data/eNCA_tweets.rds")
IOL_tweets <- read_rds("Data/Raw_Timeline_Data/IOL_tweets.rds")
SABCNews_tweets <- read_rds("Data/Raw_Timeline_Data/SABCNews_tweets.rds")
ewnupdates_tweets <- read_rds("Data/Raw_Timeline_Data/ewnupdates_tweets.rds")
```

```{r}
tweets_za_13 <- read_rds("Data/Raw_Filtered_Data/tweets_za_13.rds")
tweets_za_14 <- read_rds("Data/Raw_Filtered_Data/tweets_za_14.rds")
tweets_za_15 <- read_rds("Data/Raw_Filtered_Data/tweets_za_15.rds")
tweets_za_16 <- read_rds("Data/Raw_Filtered_Data/tweets_za_16.rds")
tweets_za_17 <- read_rds("Data/Raw_Filtered_Data/tweets_za_17.rds")
tweets_za_18 <- read_rds("Data/Raw_Filtered_Data/tweets_za_18.rds")
#tweets_za_19 <- read_rds("Data/Raw_Filtered_Data/tweets_za_19.rds")
#tweets_za_20 <- read_rds("Data/Raw_Filtered_Data/tweets_za_20.rds")
#tweets_za_21 <- read_rds("Data/Raw_Filtered_Data/tweets_za_21.rds")
```

```{r}
tweets_bloem_13 <- read_rds("Data/Raw_Filtered_Data/tweets_bloem_13.rds")
tweets_bloem_14 <- read_rds("Data/Raw_Filtered_Data/tweets_bloem_14.rds")
tweets_bloem_15 <- read_rds("Data/Raw_Filtered_Data/tweets_bloem_15.rds")
tweets_bloem_16 <- read_rds("Data/Raw_Filtered_Data/tweets_bloem_16.rds")
tweets_bloem_17 <- read_rds("Data/Raw_Filtered_Data/tweets_bloem_17.rds")
tweets_bloem_18 <- read_rds("Data/Raw_Filtered_Data/tweets_bloem_18.rds")
#tweets_bloem_19 <- read_rds("Data/Raw_Filtered_Data/tweets_bloem_19.rds")
#tweets_bloem_20 <- read_rds("Data/Raw_Filtered_Data/tweets_bloem_20.rds")
#tweets_bloem_21 <- read_rds("Data/Raw_Filtered_Data/tweets_bloem_21.rds")
```

```{r}
tweets_cpt_13 <- read_rds("Data/Raw_Filtered_Data/tweets_cpt_13.rds")
tweets_cpt_14 <- read_rds("Data/Raw_Filtered_Data/tweets_cpt_14.rds")
tweets_cpt_15 <- read_rds("Data/Raw_Filtered_Data/tweets_cpt_15.rds")
tweets_cpt_16 <- read_rds("Data/Raw_Filtered_Data/tweets_cpt_16.rds")
tweets_cpt_17 <- read_rds("Data/Raw_Filtered_Data/tweets_cpt_17.rds")
tweets_cpt_18 <- read_rds("Data/Raw_Filtered_Data/tweets_cpt_18.rds")
#tweets_cpt_19 <- read_rds("Data/Raw_Filtered_Data/tweets_cpt_19.rds")
#tweets_cpt_20 <- read_rds("Data/Raw_Filtered_Data/tweets_cpt_20.rds")
#tweets_cpt_21 <- read_rds("Data/Raw_Filtered_Data/tweets_cpt_21.rds")
```

```{r}
tweets_durb_13 <- read_rds("Data/Raw_Filtered_Data/tweets_durb_13.rds")
tweets_durb_14 <- read_rds("Data/Raw_Filtered_Data/tweets_durb_14.rds")
tweets_durb_15 <- read_rds("Data/Raw_Filtered_Data/tweets_durb_15.rds")
tweets_durb_16 <- read_rds("Data/Raw_Filtered_Data/tweets_durb_16.rds")
tweets_durb_17 <- read_rds("Data/Raw_Filtered_Data/tweets_durb_17.rds")
tweets_durb_18 <- read_rds("Data/Raw_Filtered_Data/tweets_durb_18.rds")
#tweets_durb_19 <- read_rds("Data/Raw_Filtered_Data/tweets_durb_19.rds")
#tweets_durb_20 <- read_rds("Data/Raw_Filtered_Data/tweets_durb_20.rds")
#tweets_durb_21 <- read_rds("Data/Raw_Filtered_Data/tweets_durb_21.rds")
```

```{r}
tweets_jhb_13 <- read_rds("Data/Raw_Filtered_Data/tweets_jhb_13.rds")
tweets_jhb_14 <- read_rds("Data/Raw_Filtered_Data/tweets_jhb_14.rds")
tweets_jhb_15 <- read_rds("Data/Raw_Filtered_Data/tweets_jhb_15.rds")
tweets_jhb_16 <- read_rds("Data/Raw_Filtered_Data/tweets_jhb_16.rds")
tweets_jhb_17 <- read_rds("Data/Raw_Filtered_Data/tweets_jhb_17.rds")
tweets_jhb_18 <- read_rds("Data/Raw_Filtered_Data/tweets_jhb_18.rds")
#tweets_jhb_19 <- read_rds("Data/Raw_Filtered_Data/tweets_jhb_19.rds")
#tweets_jhb_20 <- read_rds("Data/Raw_Filtered_Data/tweets_jhb_20.rds")
#tweets_jhb_21 <- read_rds("Data/Raw_Filtered_Data/tweets_jhb_21.rds")
```

```{r}
tweets_kim_13 <- read_rds("Data/Raw_Filtered_Data/tweets_kim_13.rds")
tweets_kim_14 <- read_rds("Data/Raw_Filtered_Data/tweets_kim_14.rds")
tweets_kim_15 <- read_rds("Data/Raw_Filtered_Data/tweets_kim_15.rds")
tweets_kim_16 <- read_rds("Data/Raw_Filtered_Data/tweets_kim_16.rds")
tweets_kim_17 <- read_rds("Data/Raw_Filtered_Data/tweets_kim_17.rds")
tweets_kim_18 <- read_rds("Data/Raw_Filtered_Data/tweets_kim_18.rds")
#tweets_kim_19 <- read_rds("Data/Raw_Filtered_Data/tweets_kim_19.rds")
#tweets_kim_20 <- read_rds("Data/Raw_Filtered_Data/tweets_kim_20.rds")
#tweets_kim_21 <- read_rds("Data/Raw_Filtered_Data/tweets_kim_21.rds")
```

```{r}
tweets_klerk_13 <- read_rds("Data/Raw_Filtered_Data/tweets_klerk_13.rds")
tweets_klerk_14 <- read_rds("Data/Raw_Filtered_Data/tweets_klerk_14.rds")
tweets_klerk_15 <- read_rds("Data/Raw_Filtered_Data/tweets_klerk_15.rds")
tweets_klerk_16 <- read_rds("Data/Raw_Filtered_Data/tweets_klerk_16.rds")
tweets_klerk_17 <- read_rds("Data/Raw_Filtered_Data/tweets_klerk_17.rds")
tweets_klerk_18 <- read_rds("Data/Raw_Filtered_Data/tweets_klerk_18.rds")
#tweets_klerk_19 <- read_rds("Data/Raw_Filtered_Data/tweets_klerk_19.rds")
#tweets_klerk_20 <- read_rds("Data/Raw_Filtered_Data/tweets_klerk_20.rds")
#tweets_klerk_21 <- read_rds("Data/Raw_Filtered_Data/tweets_klerk_21.rds")
```

```{r}
tweets_mbomb_13 <- read_rds("Data/Raw_Filtered_Data/tweets_mbomb_13.rds")
tweets_mbomb_14 <- read_rds("Data/Raw_Filtered_Data/tweets_mbomb_14.rds")
tweets_mbomb_15 <- read_rds("Data/Raw_Filtered_Data/tweets_mbomb_15.rds")
tweets_mbomb_16 <- read_rds("Data/Raw_Filtered_Data/tweets_mbomb_16.rds")
tweets_mbomb_17 <- read_rds("Data/Raw_Filtered_Data/tweets_mbomb_17.rds")
tweets_mbomb_18 <- read_rds("Data/Raw_Filtered_Data/tweets_mbomb_18.rds")
#tweets_mbomb_19 <- read_rds("Data/Raw_Filtered_Data/tweets_mbomb_19.rds")
#tweets_mbomb_20 <- read_rds("Data/Raw_Filtered_Data/tweets_mbomb_20.rds")
#tweets_mbomb_21 <- read_rds("Data/Raw_Filtered_Data/tweets_mbomb_21.rds")
```

```{r}
tweets_polo_13 <- read_rds("Data/Raw_Filtered_Data/tweets_polo_13.rds")
tweets_polo_14 <- read_rds("Data/Raw_Filtered_Data/tweets_polo_14.rds")
tweets_polo_15 <- read_rds("Data/Raw_Filtered_Data/tweets_polo_15.rds")
tweets_polo_16 <- read_rds("Data/Raw_Filtered_Data/tweets_polo_16.rds")
tweets_polo_17 <- read_rds("Data/Raw_Filtered_Data/tweets_polo_17.rds")
tweets_polo_18 <- read_rds("Data/Raw_Filtered_Data/tweets_polo_18.rds")
#tweets_polo_19 <- read_rds("Data/Raw_Filtered_Data/tweets_polo_19.rds")
#tweets_polo_20 <- read_rds("Data/Raw_Filtered_Data/tweets_polo_20.rds")
#tweets_polo_21 <- read_rds("Data/Raw_Filtered_Data/tweets_polo_21.rds")
```

```{r}
tweets_pe_13 <- read_rds("Data/Raw_Filtered_Data/tweets_pe_13.rds")
tweets_pe_14 <- read_rds("Data/Raw_Filtered_Data/tweets_pe_14.rds")
tweets_pe_15 <- read_rds("Data/Raw_Filtered_Data/tweets_pe_15.rds")
tweets_pe_16 <- read_rds("Data/Raw_Filtered_Data/tweets_pe_16.rds")
tweets_pe_17 <- read_rds("Data/Raw_Filtered_Data/tweets_pe_17.rds")
tweets_pe_18 <- read_rds("Data/Raw_Filtered_Data/tweets_pe_18.rds")
#tweets_pe_19 <- read_rds("Data/Raw_Filtered_Data/tweets_pe_19.rds")
#tweets_pe_20 <- read_rds("Data/Raw_Filtered_Data/tweets_pe_20.rds")
#tweets_pe_21 <- read_rds("Data/Raw_Filtered_Data/tweets_pe_21.rds")
```

# Bind Dataframes
```{r}
tweets_za <-rbind(tweets_za_18,tweets_za_17,tweets_za_16,tweets_za_15,tweets_za_14,tweets_za_13)
tweets_bloem <-rbind(tweets_bloem_18,tweets_bloem_17,tweets_bloem_16,tweets_bloem_15,tweets_bloem_14,tweets_bloem_13)
tweets_cpt <-rbind(tweets_cpt_18,tweets_cpt_17,tweets_cpt_16,tweets_cpt_15,tweets_cpt_14,tweets_cpt_13)
tweets_durb <-rbind(tweets_durb_18,tweets_durb_17,tweets_durb_16,tweets_durb_15,tweets_durb_14,tweets_durb_13)
tweets_jhb <-rbind(tweets_jhb_18,tweets_jhb_17,tweets_jhb_16,tweets_jhb_15,tweets_jhb_14,tweets_jhb_13)
tweets_kim <-rbind(tweets_kim_18,tweets_kim_17,tweets_kim_16,tweets_kim_15,tweets_kim_14,tweets_kim_13)
tweets_klerk <-rbind(tweets_klerk_18,tweets_klerk_17,tweets_klerk_16,tweets_klerk_15,tweets_klerk_14,tweets_klerk_13)
tweets_mbomb <-rbind(tweets_mbomb_18,tweets_mbomb_17,tweets_mbomb_16,tweets_mbomb_15,tweets_mbomb_14,tweets_mbomb_13)
tweets_polo <-rbind(tweets_polo_18,tweets_polo_17,tweets_polo_16,tweets_polo_15,tweets_polo_14,tweets_polo_13)
tweets_pe <-rbind(tweets_pe_18,tweets_pe_17,tweets_pe_16,tweets_pe_15,tweets_pe_14,tweets_pe_13)

```

# Performing transformations

## cleaning data (text)
```{r}
#Separate date column
News24_tweets <- separate(News24_tweets, created_at, into=c("date","time"), sep=" ") %>% mutate(date = as.Date(date)) %>% select(date, time, text, source, favorite_count, retweet_count)
timesLIVE_tweets <- separate(TimesLIVE_tweets, created_at, into=c("date","time"), sep=" ") %>% mutate(date = as.Date(date)) %>% select(date, time, text, source, favorite_count, retweet_count)
Ewnupdate_tweets <- separate(ewnupdates_tweets, created_at, into=c("date","time"), sep=" ") %>% mutate(date = as.Date(date)) %>% select(date, time, text, source, favorite_count, retweet_count)
IOL_tweets <- separate(IOL_tweets, created_at, into=c("date","time"), sep=" ") %>% mutate(date = as.Date(date)) %>% select(date, time, text, source, favorite_count, retweet_count)
eNCA_tweets <- separate(eNCA_tweets, created_at, into=c("date","time"), sep=" ") %>% mutate(date = as.Date(date)) %>% select(date, time, text, source, favorite_count, retweet_count)
SABC_tweets <- separate(SABCNews_tweets, created_at, into=c("date","time"), sep=" ") %>% mutate(date = as.Date(date)) %>% select(date, time, text, source, favorite_count, retweet_count)


#Subset data into 3 week range 1 June - 21 June

myfunc <- function(x,y){News24_tweets[News24_tweets$date >= x & News24_tweets$date <= y,]}

DATE1 <- as.Date("2020-06-01")
DATE2 <- as.Date("2020-06-21")

News24_tweets <- myfunc(DATE1,DATE2)
timesLIVE_tweets <- myfunc(DATE1,DATE2)
Ewnupdate_tweets <- myfunc(DATE1,DATE2)
IOL_tweets <- myfunc(DATE1,DATE2)
eNCA_tweets <- myfunc(DATE1,DATE2)
SABC_tweets <- myfunc(DATE1,DATE2)


```

# AddItional cleaning per dataframe
```{r}
#News24 cleaning
tweets.News24 <- News24_tweets %>% select(text)

tweets.News24_clean <- unique(tweets.News24)
tweets.News24_clean$stripped_text <- tolower(tweets.News24_clean$text)


tweets.News24_clean$stripped_text <- gsub("http[^[:space:]]*", " ", tweets.News24_clean$stripped_text) # remove links
tweets.News24_clean$stripped_text <- gsub("@\\S*","", tweets.News24_clean$stripped_text) # remove mentons
tweets.News24_clean$stripped_text <- gsub("amp","", tweets.News24_clean$stripped_text) 
tweets.News24_clean$stripped_text <- gsub("[\r\n]","", tweets.News24_clean$stripped_text)
tweets.News24_clean$stripped_text <- gsub("[[:punct:]]"," ", tweets.News24_clean$stripped_text) #emove punctuation
tweets.News24_clean$stripped_text = gsub("[ |\t]{2,}", " ", tweets.News24_clean$stripped_text) # remove tabs
tweets.News24_clean$stripped_text = gsub("^ ", "", tweets.News24_clean$stripped_text) # leading blanks
tweets.News24_clean$stripped_text = gsub(" $", "", tweets.News24_clean$stripped_text) #laggin blanks
tweets.News24_clean$stripped_text = gsub(" +", " ", tweets.News24_clean$stripped_text) #general spaces

tweets.News24_clean <- tweets.News24_clean %>% select(stripped_text) 

tweets.News24_stem <- tweets.News24_clean %>% unnest_tokens(word, stripped_text)

clean_tweets.News24 <- tweets.News24_stem %>% anti_join(stop_words)

clean_tweets.News24
```


```{r}
#TimesLIVE cleaning
tweets.TimesLIVE <- TimesLIVE_tweets %>% select(text)

tweets.TimesLIVE_clean <- unique(tweets.TimesLIVE)
tweets.TimesLIVE_clean$stripped_text <- tolower(tweets.TimesLIVE_clean$text)


tweets.TimesLIVE_clean$stripped_text <- gsub("http.+ |http.+$", " ", tweets.TimesLIVE_clean$stripped_text) # remove links
tweets.TimesLIVE_clean$stripped_text <- gsub("@\\S*","", tweets.TimesLIVE_clean$stripped_text) # remove mentons
tweets.TimesLIVE_clean$stripped_text <- gsub("amp","", tweets.TimesLIVE_clean$stripped_text) 
tweets.TimesLIVE_clean$stripped_text <- gsub("[\r\n]","", tweets.TimesLIVE_clean$stripped_text)
tweets.TimesLIVE_clean$stripped_text <- gsub("[[:punct:]]"," ", tweets.TimesLIVE_clean$stripped_text) #emove punctuation
tweets.TimesLIVE_clean$stripped_text = gsub("[ |\t]{2,}", " ", tweets.TimesLIVE_clean$stripped_text) # remove tabs
tweets.TimesLIVE_clean$stripped_text = gsub("^ ", "", tweets.TimesLIVE_clean$stripped_text) # leading blanks
tweets.TimesLIVE_clean$stripped_text = gsub(" $", "", tweets.TimesLIVE_clean$stripped_text) #laggin blanks
tweets.TimesLIVE_clean$stripped_text = gsub(" +", " ", tweets.TimesLIVE_clean$stripped_text) #general spaces

tweets.TimesLIVE_stem <- tweets.TimesLIVE_clean %>% select(stripped_text) %>% unnest_tokens(word, stripped_text)

clean_tweets.TimesLIVE <- tweets.TimesLIVE_stem %>% anti_join(stop_words)

clean_tweets.TimesLIVE
```

```{r}
#eNCA cleaning
tweets.eNCA <- eNCA_tweets %>% select(text)

tweets.eNCA_clean <- unique(tweets.eNCA)
tweets.eNCA_clean$stripped_text <- tolower(tweets.eNCA_clean$text)


tweets.eNCA_clean$stripped_text <- gsub("http.+ |http.+$", " ", tweets.eNCA_clean$stripped_text) # remove links
tweets.eNCA_clean$stripped_text <- gsub("@\\S*","", tweets.eNCA_clean$stripped_text) # remove mentons
tweets.eNCA_clean$stripped_text <- gsub("amp","", tweets.eNCA_clean$stripped_text) 
tweets.eNCA_clean$stripped_text <- gsub("[\r\n]","", tweets.eNCA_clean$stripped_text)
tweets.eNCA_clean$stripped_text <- gsub("[[:punct:]]"," ", tweets.eNCA_clean$stripped_text) #emove punctuation
tweets.eNCA_clean$stripped_text = gsub("[ |\t]{2,}", " ", tweets.eNCA_clean$stripped_text) # remove tabs
tweets.eNCA_clean$stripped_text = gsub("^ ", "", tweets.eNCA_clean$stripped_text) # leading blanks
tweets.eNCA_clean$stripped_text = gsub(" $", "", tweets.eNCA_clean$stripped_text) #laggin blanks
tweets.eNCA_clean$stripped_text = gsub(" +", " ", tweets.eNCA_clean$stripped_text) #general spaces

tweets.eNCA_stem <- tweets.eNCA_clean %>% select(stripped_text) %>% unnest_tokens(word, stripped_text)

clean_tweets.eNCA <- tweets.eNCA_stem %>% anti_join(stop_words)

clean_tweets.eNCA
```


```{r}
#IOL cleaning
tweets.IOL <- IOL_tweets %>% select(text)

tweets.IOL_clean <- unique(tweets.IOL)
tweets.IOL_clean$stripped_text <- tolower(tweets.IOL_clean$text)


tweets.IOL_clean$stripped_text <- gsub("http.+ |http.+$", " ", tweets.IOL_clean$stripped_text) # remove links
tweets.IOL_clean$stripped_text <- gsub("@\\S*","", tweets.IOL_clean$stripped_text) # remove mentons
tweets.IOL_clean$stripped_text <- gsub("amp","", tweets.IOL_clean$stripped_text) 
tweets.IOL_clean$stripped_text <- gsub("[\r\n]","", tweets.IOL_clean$stripped_text)
tweets.IOL_clean$stripped_text <- gsub("[[:punct:]]"," ", tweets.IOL_clean$stripped_text) #emove punctuation
tweets.IOL_clean$stripped_text = gsub("[ |\t]{2,}", " ", tweets.IOL_clean$stripped_text) # remove tabs
tweets.IOL_clean$stripped_text = gsub("^ ", "", tweets.IOL_clean$stripped_text) # leading blanks
tweets.IOL_clean$stripped_text = gsub(" $", "", tweets.IOL_clean$stripped_text) #laggin blanks
tweets.IOL_clean$stripped_text = gsub(" +", " ", tweets.IOL_clean$stripped_text) #general spaces

tweets.IOL_stem <- tweets.IOL_clean %>% select(stripped_text) %>% unnest_tokens(word, stripped_text)

clean_tweets.IOL <- tweets.IOL_stem %>% anti_join(stop_words)

clean_tweets.IOL
```

```{r}
#ewn cleaning
tweets.ewn <- ewnupdates_tweets %>% select(text)

tweets.ewn_clean <- unique(tweets.ewn)
tweets.ewn_clean$stripped_text <- tolower(tweets.ewn_clean$text)


tweets.ewn_clean$stripped_text <- gsub("http.+ |http.+$", " ", tweets.ewn_clean$stripped_text) # remove links
tweets.ewn_clean$stripped_text <- gsub("@\\S*","", tweets.ewn_clean$stripped_text) # remove mentons
tweets.ewn_clean$stripped_text <- gsub("amp","", tweets.ewn_clean$stripped_text) 
tweets.ewn_clean$stripped_text <- gsub("[\r\n]","", tweets.ewn_clean$stripped_text)
tweets.ewn_clean$stripped_text <- gsub("[[:punct:]]"," ", tweets.ewn_clean$stripped_text) #emove punctuation
tweets.ewn_clean$stripped_text = gsub("[ |\t]{2,}", " ", tweets.ewn_clean$stripped_text) # remove tabs
tweets.ewn_clean$stripped_text = gsub("^ ", "", tweets.ewn_clean$stripped_text) # leading blanks
tweets.ewn_clean$stripped_text = gsub(" $", "", tweets.ewn_clean$stripped_text) #laggin blanks
tweets.ewn_clean$stripped_text = gsub(" +", " ", tweets.ewn_clean$stripped_text) #general spaces

tweets.ewn_stem <- tweets.ewn_clean %>% select(stripped_text) %>% unnest_tokens(word, stripped_text)

clean_tweets.ewn <- tweets.ewn_stem %>% anti_join(stop_words)

clean_tweets.ewn
```

```{r}
#SABCNews cleaning
tweets.SABCNews <- SABCNews_tweets %>% select(text)

tweets.SABCNews_clean <- unique(tweets.SABCNews)
tweets.SABCNews_clean$stripped_text <- tolower(tweets.SABCNews_clean$text)


tweets.SABCNews_clean$stripped_text <- gsub("http.+ |http.+$", " ", tweets.SABCNews_clean$stripped_text) # remove links
tweets.SABCNews_clean$stripped_text <- gsub("@\\S*","", tweets.SABCNews_clean$stripped_text) # remove mentons
tweets.SABCNews_clean$stripped_text <- gsub("amp","", tweets.SABCNews_clean$stripped_text) 
tweets.SABCNews_clean$stripped_text <- gsub("[\r\n]","", tweets.SABCNews_clean$stripped_text)
tweets.SABCNews_clean$stripped_text <- gsub("[[:punct:]]"," ", tweets.SABCNews_clean$stripped_text) #emove punctuation
tweets.SABCNews_clean$stripped_text = gsub("[ |\t]{2,}", " ", tweets.SABCNews_clean$stripped_text) # remove tabs
tweets.SABCNews_clean$stripped_text = gsub("^ ", "", tweets.SABCNews_clean$stripped_text) # leading blanks
tweets.SABCNews_clean$stripped_text = gsub(" $", "", tweets.SABCNews_clean$stripped_text) #laggin blanks
tweets.SABCNews_clean$stripped_text = gsub(" +", " ", tweets.SABCNews_clean$stripped_text) #general spaces

tweets.SABCNews_stem <- tweets.SABCNews_clean %>% select(stripped_text) %>% unnest_tokens(word, stripped_text)

clean_tweets.SABCNews <- tweets.SABCNews_stem  %>% anti_join(stop_words)

clean_tweets.SABCNews
```


```{r}
# AFFIN Sentiment Analysis
# this example uses 

#bing_news24 <- clean_tweets.News24 %>% inner_join(get_sentiments("bing")) %>% count(word, sentiment, sort = TRUE) %>% ungroup()

#bing_news24

afinn_news24 <- clean_tweets.News24 %>% inner_join(get_sentiments("afinn")) %>% count(word, value, sort = TRUE) %>% ungroup()
afinn_news24

afinn_TimesLIVE <- clean_tweets.TimesLIVE %>% inner_join(get_sentiments("afinn")) %>% count(word, value, sort = TRUE) %>% ungroup()
afinn_TimesLIVE

afinn_eNCA <- clean_tweets.eNCA %>% inner_join(get_sentiments("afinn")) %>% count(word, value, sort = TRUE) %>% ungroup()
afinn_eNCA

afinn_IOL <- clean_tweets.IOL %>% inner_join(get_sentiments("afinn")) %>% count(word, value, sort = TRUE) %>% ungroup()
afinn_IOL

afinn_ewn <- clean_tweets.ewn %>% inner_join(get_sentiments("afinn")) %>% count(word, value, sort = TRUE) %>% ungroup()
afinn_ewn

afinn_SABCNews <- clean_tweets.SABCNews %>% inner_join(get_sentiments("afinn")) %>% count(word, value, sort = TRUE) %>% ungroup()
afinn_SABCNews

```


# Topic Analysis
```{r}
# topic modeling using LDA
# example on rpubs.com

corpusNews24 <- Corpus(VectorSource(tweets.News24_clean))
corpusNews24 <- tm_map(corpusNews24, removeWords, stopwords("english"))  
corpusNews24 <- tm_map(corpusNews24, removeNumbers)
corpusNews24 <- tm_map(corpusNews24, stemDocument)
#Remove search terms -->
dtmNews24 = DocumentTermMatrix(corpusNews24)

corpusTimesLIVE <- Corpus(VectorSource(clean_tweets.TimesLIVE))
dtmTimesLIVE = DocumentTermMatrix(corpusTimesLIVE)

corpuseNCA <- Corpus(VectorSource(clean_tweets.eNCA))
dtmeNCA = DocumentTermMatrix(corpuseNCA)

corpusIOL <- Corpus(VectorSource(clean_tweets.IOL))
dtmIOL = DocumentTermMatrix(corpusIOL)

corpusewn <- Corpus(VectorSource(clean_tweets.ewn))
dtmewn = DocumentTermMatrix(corpusewn)

corpusSABCNews <- Corpus(VectorSource(clean_tweets.SABCNews))
dtmSABCNews = DocumentTermMatrix(corpusSABCNews)
```


```{r}
#News24 LDA topic analysis

#LDA model with 2 topics selected
News24_lda_2 = LDA(dtmNews24, k = 2, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#LDA model with 5 topics selected
News24_lda_5 = LDA(dtmNews24, k = 5, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#LDA model with 10 topics selected
News24_lda_10 = LDA(dtmNews24, k = 10, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#News24_lda_20 = LDA(dtmNews24, k = 20, method = 'Gibbs', 
#          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
#                         thin = 500, burnin = 4000, iter = 2000))


#News24_lda_50 = LDA(dtmNews24, k = 50, method = 'Gibbs', control = list(seed = 1234))

# comparing total tweets by medias vs the amount that was related to COVID-19 COULD THIS BE SEEN AS TOPIC ANALYSIS???????
#start
#end
#lda_2
#lda_5
#lda_10
```


```{r}
#LDA model with 2 topics selected
TimesLIVE_lda_5 = LDA(dtmTimesLIVE, k = 5, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#LDA model with 2 topics selected
TimesLIVE_lda_2 = LDA(dtmTimesLIVE, k = 2, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#LDA model with 10 topics selected
TimesLIVE_lda_10 = LDA(dtmTimesLIVE, k = 10, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))
```

```{r}
#LDA model with 2 topics selected
eNCA_lda_5 = LDA(dtmeNCA, k = 5, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#LDA model with 2 topics selected
eNCA_lda_2 = LDA(dtmeNCA, k = 2, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#LDA model with 10 topics selected
eNCA_lda_10 = LDA(dtmeNCA, k = 10, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))
```

```{r}
#LDA model with 2 topics selected
IOL_lda_5 = LDA(dtmIOL, k = 5, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#LDA model with 2 topics selected
IOL_lda_2 = LDA(dtmIOL, k = 2, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#LDA model with 10 topics selected
IOL_lda_10 = LDA(dtmIOL, k = 10, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))
```

```{r}
#LDA model with 2 topics selected
ewn_lda_5 = LDA(dtmewn, k = 5, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#LDA model with 2 topics selected
ewn_lda_2 = LDA(dtmewn, k = 2, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#LDA model with 10 topics selected
ewn_lda_10 = LDA(dtmewn, k = 10, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))
```

```{r}
#LDA model with 2 topics selected
SABCNews_lda_5 = LDA(dtmSABCNews, k = 5, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#LDA model with 2 topics selected
SABCNews_lda_2 = LDA(dtmSABCNews, k = 2, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))

#LDA model with 10 topics selected
SABCNews_lda_10 = LDA(dtmSABCNews, k = 10, method = 'Gibbs', 
          control = list(nstart = 5, seed = list(1505,99,36,56,88), best = TRUE, 
                         thin = 500, burnin = 4000, iter = 2000))
```


```{r}
#Top 5 terms or words under each topic
top10terms_2 = as.matrix(terms(News24_lda_2,5))
top10terms_5 = as.matrix(terms(News24_lda_5,5))
top10terms_10 = as.matrix(terms(News24_lda_10,5))
#top10terms_20 = as.matrix(terms(News24_lda_20,5))
#top10terms_50 = as.matrix(terms(News24_lda_50,5))

top10terms_2
top10terms_5
top10terms_10
#top10terms_20
#top10terms_50

```

```{r}
#Topics found out by our model:
lda.topics_2 = as.matrix(topics(News24_lda_2))
lda.topics_5 = as.matrix(topics(News24_lda_5))
lda.topics_10 = as.matrix(topics(News24_lda_10))
#write.csv(lda.topics_5,file = paste('LDAGibbs',5,'DocsToTopics.csv'))
#write.csv(lda.topics_2,file = paste('LDAGibbs',2,'DocsToTopics.csv'))
#write.csv(lda.topics_10,file = paste('LDAGibbs',10,'DocsToTopics.csv'))

summary(as.factor(lda.topics_2[,1]))
summary(as.factor(lda.topics_5[,1]))
summary(as.factor(lda.topics_10[,1]))
```


```{r}
#document wise probability of each topic.
topicprob_2 = as.matrix(News24_lda_2@gamma)
topicprob_5 = as.matrix(News24_lda_5@gamma)
topicprob_10 = as.matrix(News24_lda_10@gamma)

#write.csv(topicprob_5, file = paste('LDAGibbs', 5, 'DoctToTopicProb.csv'))
#write.csv(topicprob_2, file = paste('LDAGibbs', 2, 'DoctToTopicProb.csv'))
#write.csv(topicprob_10, file = paste('LDAGibbs', 10, 'DoctToTopicProb.csv'))

head(topicprob_2,1)
head(topicprob_5,1)
head(topicprob_10,1)
```
#STM Topic Modelling

```{r}
library(stm)
```

```{r}
processed <- textProcessor(tweets.News24_clean$stripped_text, metadata = News24_tweets)
```

```{r}
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)

```

```{r}
docs <- out$documents
vocab <- out$vocab
meta <-out$meta
```


```{r}
First_STM <- stm(documents = out$documents, vocab = out$vocab,
              K = 10, max.em.its = 75, data = out$meta,
              init.type = "Spectral", verbose = FALSE)

Second_STM <- stm(documents = out$documents, vocab = out$vocab,
              K = 15, max.em.its = 75, data = out$meta,
              init.type = "Spectral", verbose = FALSE)

Third_STM <- stm(documents = out$documents, vocab = out$vocab,
              K = 20, max.em.its = 75, data = out$meta,
              init.type = "Spectral", verbose = FALSE)
```


```{r}
plot(First_STM)
plot(Second_STM)
plot(Third_STM)
```
```


```{r}
# Other Analysis

# Frequency of tweets by day
 #%>% group_by(date)
News24_freq <- News24_tweets %>% summarise(News24 = n())
timesLIVE_freq <- timesLIVE_tweets %>% summarise(timesLIVE = n())
Ewnupdate_freq <- Ewnupdate_tweets %>% summarise(Ewnupdate = n())
IOL_freq <- IOL_tweets %>% summarise(IOL = n())
eNCA_freq <- eNCA_tweets %>% summarise(eNCA = n()) 
SABC_freq <- SABC_tweets %>% summarise(SABC = n())

# sum days and join together, then send to visualisation
#Frequency_tweets <- cbind(News24_freq,timesLIVE_freq,Ewnupdate_freq,IOL_freq,eNCA_freq,SABC_freq)

#retweet count 
News24_retweet <- News24_tweets %>% summarise(News24 = sum(retweet_count))
timesLIVE_retweet <- timesLIVE_tweets %>% summarise(timesLIVE = sum(retweet_count))
Ewnupdate_retweet <- Ewnupdate_tweets %>% summarise(Ewnupdate = sum(retweet_count))
IOL_retweet <- IOL_tweets %>% summarise(IOL = sum(retweet_count))
eNCA_retweet <- eNCA_tweets %>% summarise(eNCA = sum(retweet_count)) 
SABC_retweet <- SABC_tweets %>% summarise(SABC = sum(retweet_count))

# sum days and join together, then send to visualisation
#retweet_tweets <- cbind(News24_retweet,timesLIVE_retweet,Ewnupdate_retweet,IOL_retweet,eNCA_retweet,SABC_retweet)

# like count
News24_likes <- News24_tweets %>% summarise(News24 = sum(favorite_count))
timesLIVE_likes <- timesLIVE_tweets %>% summarise(timesLIVE = sum(favorite_count))
Ewnupdate_likes <- Ewnupdate_tweets %>% summarise(Ewnupdate = sum(favorite_count))
IOL_likes <- IOL_tweets %>% summarise(IOL = sum(favorite_count))
eNCA_likes <- eNCA_tweets %>% summarise(eNCA = sum(favorite_count)) 
SABC_likes <- SABC_tweets %>% summarise(SABC = sum(favorite_count))

# sum days and join together, then send to visualisation
#Likes_tweets <- cbind(News24_likes,timesLIVE_likes,Ewnupdate_likes,IOL_likes,eNCA_likes,SABC_likes)


# Comparing likes and retweets i.e maby retweets per like or compaing stats of non-covid to covid
#start
#end

```


```{r}
# Other Analysis

# Frequency of tweets by day
News24_freq <- News24_tweets %>% summarise(News24 = n())

timesLIVE_freq <- timesLIVE_tweets %>% summarise(timesLIVE = n())

Ewnupdate_freq <- Ewnupdate_tweets %>% summarise(Ewnupdate = n())

IOL_freq <- IOL_tweets %>% summarise(IOL = n())

eNCA_freq <- eNCA_tweets %>% summarise(eNCA = n()) 

SABC_freq <- SABC_tweets %>% summarise(SABC = n())

# sum days and join together, then send to visualisation
#Frequency_tweets <- cbind(News24_freq,timesLIVE_freq,Ewnupdate_freq,IOL_freq,eNCA_freq,SABC_freq)

#retweet count 
News24_retweet <- News24_tweets %>% summarise(News24 = sum(retweet_count))

timesLIVE_retweet <- timesLIVE_tweets %>% summarise(timesLIVE = sum(retweet_count))

Ewnupdate_retweet <- Ewnupdate_tweets %>% summarise(Ewnupdate = sum(retweet_count))

IOL_retweet <- IOL_tweets %>% summarise(IOL = sum(retweet_count))

eNCA_retweet <- eNCA_tweets %>% summarise(eNCA = sum(retweet_count)) 

SABC_retweet <- SABC_tweets %>% summarise(SABC = sum(retweet_count))

# sum days and join together, then send to visualisation
#retweet_tweets <- cbind(News24_retweet,timesLIVE_retweet,Ewnupdate_retweet,IOL_retweet,eNCA_retweet,SABC_retweet)

# like count
News24_likes <- News24_tweets %>% summarise(News24 = sum(favorite_count))

timesLIVE_likes <- timesLIVE_tweets %>% summarise(timesLIVE = sum(favorite_count))

Ewnupdate_likes <- Ewnupdate_tweets %>% summarise(Ewnupdate = sum(favorite_count))

IOL_likes <- IOL_tweets %>% summarise(IOL = sum(favorite_count))

eNCA_likes <- eNCA_tweets %>% summarise(eNCA = sum(favorite_count)) 

SABC_likes <- SABC_tweets %>% summarise(SABC = sum(favorite_count))

# sum days and join together, then send to visualisation
#Likes_tweets <- cbind(News24_likes,timesLIVE_likes,Ewnupdate_likes,IOL_likes,eNCA_likes,SABC_likes)


# Comparing likes and retweets i.e maby retweets per like or compaing stats of non-covid to covid
#start
#end

```

```{r}
#count the total number of tweets for each media house
News24_freq <- nrow(News24_freq)
timesLIVE_freq <- nrow(timesLIVE_freq)
Ewnupdate_freq <- nrow(Ewnupdate_freq)
IOL_freq <- nrow(IOL_freq)
eNCA_freq <- nrow(eNCA_freq)
SABC_freq <- nrow(SABC_freq)

News24_freq
timesLIVE_freq
Ewnupdate_freq
IOL_freq
eNCA_freq
SABC_freq
```


## Done

# Exporting transformation results
```{r echo=TRUE}
#write_rds(News24_tweets, "Results/News24_tweets.rds")
#write_rds(TimesLIVE_tweets, "Results/TimesLIVE_tweets.rds")
#write_rds(eNCA_tweets, "Results/eNCA_tweets.rds")
#write_rds(IOL_tweets,"Results/IOL_tweets.rds")
#write_rds(SABCNews_tweets,"Results/SABCNews_tweets.rds")
#write_rds(ewnupdates_tweets,"Results/ewnupdates_tweets")
```

```{r}




```



```

## Exporting results
```{r}

```